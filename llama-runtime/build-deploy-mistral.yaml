---
# BuildConfig for Docker strategy
apiVersion: build.openshift.io/v1
kind: BuildConfig
metadata:
  name: mma-ai
  labels:
    app: mma-ai
    app.kubernetes.io/component: mma-ai
    app.kubernetes.io/instance: mma-ai
    app.kubernetes.io/name: mma-ai
    app.kubernetes.io/part-of: mma-ai-app
spec:
  output:
    to:
      kind: ImageStreamTag
      name: mma-ai:latest
  source:
    contextDir: llama-runtime
    git:
      uri: https://github.com/Milstein/mma-ai-milson
      ref: main
    type: Git
  strategy:
    dockerStrategy:
      dockerfilePath: Dockerfile  # Dockerfile in llama-runtime directory
    type: Docker
  triggers:
  - github:
      secretReference:
        name: mma-ai-github-webhook-secret  # Replace with your webhook secret
    type: GitHub
  - imageChange: {}
    type: ImageChange
  - type: ConfigChange
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llama-models
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: ocs-external-storagecluster-ceph-rbd
  volumeMode: Filesystem
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: mma-ai
    app.kubernetes.io/component: mma-ai
    app.kubernetes.io/instance: mma-ai
    app.kubernetes.io/name: mma-ai
    app.kubernetes.io/part-of: mma-ai-app
  name: mma-ai
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: mma-ai
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: mma-ai
        deployment: mma-ai
    spec:
      initContainers:
        - name: fetch-model-data
          image: ubi8
          volumeMounts:
            - name: llama-models
              mountPath: /models
          command:
            - sh
            - '-c'
            - |
              if [ ! -f /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf ] ; then
                curl -L https://huggingface.co/SanctumAI/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/mistral-7b-instruct-v0.3.Q4_K_M.gguf --output /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf
              else
                echo "model /models/mistral-7b-instruct-v0.3.Q4_K_M.gguf already present"
              fi
          resources:
            limits:
              cpu: '3'
              memory: 5Gi
            requests:
              cpu: '2'
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
      containers:
        - image: quay.io/milstein/llama-runtime-ubi:latest
          imagePullPolicy: Always
          args: ["-m", "/models/mistral-7b-instruct-v0.3.Q4_K_M.gguf", "--prio", "3", "-c", "4096","-b", "32", "-t", "48", "-n", "-1"]
          name: mma-ai
          ports:
            - containerPort: 8080
              name: http
          resources:
            limits:
              cpu: '11'
              memory: 33Gi
              nvidia.com/gpu: '1'
            requests:
              cpu: '10'
              memory: 32Gi
              nvidia.com/gpu: '1'
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
            - name: llama-models
              mountPath: /models
          readinessProbe:
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 1
            periodSeconds: 20
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /
              port: 8080
              scheme: HTTP
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
        # - name: llama-models
        #   emptyDir:
        #     medium: Memory
        - name: llama-models
          persistentVolumeClaim:
            claimName: llama-models
---
# ImageStream for the application
apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  name: mma-ai
  labels:
    app: mma-ai
    app.kubernetes.io/component: mma-ai
    app.kubernetes.io/instance: mma-ai
    app.kubernetes.io/name: mma-ai
    app.kubernetes.io/part-of: mma-ai-app
spec:
  lookupPolicy:
    local: true
---
# Route to expose the service externally
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: "true"
  name: mma-ai
  labels:
    app: mma-ai
    app.kubernetes.io/component: mma-ai
    app.kubernetes.io/instance: mma-ai
    app.kubernetes.io/name: mma-ai
    app.kubernetes.io/part-of: mma-ai-app
spec:
  port:
    targetPort: http
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: edge
  to:
    kind: Service
    name: mma-ai
    weight: 100
  wildcardPolicy: None
---
apiVersion: v1
data:
  WebHookSecretKey: YzA5NWE1Mzg3YjM4ZTI2NA==
kind: Secret
metadata:
  labels:
    app: mma-ai
    app.kubernetes.io/component: mma-ai
    app.kubernetes.io/instance: mma-ai
    app.kubernetes.io/name: mma-ai
    app.kubernetes.io/part-of: mma-ai-app
  name: mma-ai-github-webhook-secret
type: Opaque
---
# Service to expose the application internally
apiVersion: v1
kind: Service
metadata:
  name: mma-ai
  labels:
    app: mma-ai
    app.kubernetes.io/component: mma-ai
    app.kubernetes.io/instance: mma-ai
    app.kubernetes.io/name: mma-ai
    app.kubernetes.io/part-of: mma-ai-app
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app: mma-ai
    deployment: mma-ai
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: "llama-service"
  labels:
    app: mma-ai
    app.kubernetes.io/component: mma-ai
    app.kubernetes.io/instance: mma-ai
    app.kubernetes.io/name: mma-ai
    app.kubernetes.io/part-of: mma-ai-app
spec:
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app: mma-ai
    deployment: mma-ai
  sessionAffinity: None
  type: ClusterIP
